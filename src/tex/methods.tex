\section{Methods} \label{sec:methods}

\subsection{Statistical Framework} \label{sec:statistical_framework}

We employ the typical hierarchical Bayesian inference framework to infer the properties of the population of merging compact binaries given a catalog of observations. The rate of compact binary mergers is modeled as an inhomogeneous Poisson point process \citep{10.1093/mnras/stz896}, with the merger rate per comoving volume $V_c$ \citep{astro-ph/9905116}, source-frame time $T_\text{src}$ and binary parameters $\theta$ defined as: 

\begin{equation} \label{eq:rate}
    \frac{dN}{dV_cdt_\mathrm{src}d\theta} = \frac{dN}{dV_cdt_\mathrm{src}} p(\theta | \Lambda) = \mathcal{R} p(\theta | \Lambda)
\end{equation}

\noindent with $p(\theta | \Lambda)$ the population model, $\mathcal{R}$ the merger rate, and $\Lambda$ the set of population hyperparameters. Following other population studies \citep{10.1093/mnras/stz896,2007.05579,2010.14533,2111.03634}, we use the hierarchical likelihood that incorporates selection effects and marginalizes over the merger rate as: 

\begin{equation} \label{eq:likelihood}
    \mathcal{L}(\bm{d} | \Lambda) \propto \frac{1}{\xi(\Lambda)} \prod_{i=1}^{N_\mathrm{det}} \int d\theta \mathcal{L}(d_i | \theta) p(\theta | \Lambda)
\end{equation}

\noindent Above, $\bm{d}$ is the set of data containing $N_\mathrm{det}$ observed events, $\mathcal{L}(d_i | \theta)$ is the individual event likelihood function for the $i$th event given parameters $\theta$ and $\xi(\Lambda)$ is the fraction of merging binaries we expect to detect, given a population described by $\Lambda$. The integral of the individual event likelihoods marginalizes over the uncertainty in each event's binary parameter estimation, and is calculated with Monte Carlo integration and by importance sampling, reweighing each set of posterior samples to the likelihood. The detection fraction is calculated with:

\begin{equation} \label{eq:detfrac}
    \xi(\Lambda) = \int d\theta p_\mathrm{det}(\theta) p(\theta | \Lambda)
\end{equation}

\noindent with $p_\mathrm{det}(\theta)$ the probability of detecting a binary merger with parameters $\theta$. We calculate this fraction using simulated compact merger signals that were evaluated with the same search algorithms that produced the catalog of observations. With the signals that were successfully detected, we again use Monte Carlo integration to get the overall detection efficiency, $\xi(\Lambda)$.


To model different subpopulations that could exist in the population, we use discrete latent variables that probabilistically associate each binary merger with different models. To model $K$ subpopulations in a catalog of $N_\mathrm{det}$ detections, we add a latent variable $q_i$ for each merger that can be $K$ different discrete values, each associated with a separate model, $p_{k}(\theta | \Lambda)$, and hyperparameters, $\Lambda_k$. Evaluating the model (or hyper-prior) for the $i^\mathrm{th}$ event with binary parameters, $\theta_i$, given latent variable $q_i$ and hyperparameters $\Lambda_k$, we have:

\begin{equation} \label{eq:latent}
    p(\theta_i | \Lambda, q_i) = p_{k=q_i}(\theta_i | \Lambda_{k=q_i})
\end{equation}

\noindent To construct our probabilistic model, we first sample $p_k \sim \mathcal{D}(K)$, from a K-dimensional Dirichlet distribution of equal weights, representing the astrophysical branching ratios of each subpopulation. Then each of the $N_\mathrm{det}$ discrete latent variables are sampled from a categorical distribution with each category $k$ having probability, $p_k$. Within the \textsc{NumPyro} \citep{1810.09538,1912.11554} probabilistic programming language, we use the implementation of the \texttt{DiscreteHMCGibbs} \citep{Liu1996PeskunsTA} to sample the discrete latent variables, while using the \texttt{NUTS} \citep{1111.4246} sampler for continuous variables. While this approach may seem computationally expensive, we find that the conditional distributions over discrete latent variables enable Gibbs sampling with similar costs and speeds to the equivalent approach that marginalizes over each discrete latent variable, $q_i$. We find the same results with either approach and only slight performance differences that depend on specific model specifications, and thus opt for the approach without marginalization. This method also has the advantage that we get posterior distributions on each event's subpopulation assignment without extra steps.

\subsection{Astrophysical Mixture Models} \label{sec:astromodels}

\begin{itemize}
    \item describe the specific models we use in the paper
    \item point towards implementations in GWInferno and code etc
\end{itemize}

For this study, we focus on one collection of primary mass and spin models to break the BBH population into three different subpopulations. Throughout this work, we refer to these three different subpopulations by their mass models as \textsc{Low-Mass Peak}, \textsc{Mid-Mass Peak}, and \textsc{Continuum}. 

\subsubsection{Mass Models}

We chose to model the primary mass as two truncated gaussian peaks plus a basis spline (b-spline) function, a similar model to the \textsc{Mulit Peak} mass model in . 

Given the recent evidence for a $~10 M_{\odot}$ and $~35 M_{\odot}$ peak in the BBH primary mass distribution \citep{2111.03634, 2022ApJ...928..155T, 10.3847/2041-8213/aa9bf6, 10.3847/1538-4357/aab34c, 10.3847/2041-8213/ab3800, 2021ApJ...913L...7A}, we chose to use a similar primary mass model to the \textsc{Multi Peak} model in \cite{2021ApJ...913L...7A}, except we replace their power law component with a basis spline function (b-spline), which is a non-parametric model. We did this in order to avoid the model dependent biases on our resulting total distribution that we noticed occurred when we used a power law.  
