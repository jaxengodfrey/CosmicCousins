\section{Methods} \label{sec:methods}

\begin{itemize}
    \item VERY quick basic point to hierarchical Bayesian inference details
    \item Introduce discrete latent variables / marginalized mixture models
    \item describe equivalence of fitting discrete latent variables to marginalized mixture models
    \item point out GWInferno and code etc
    \item describe the specific models we use in the paper
\end{itemize}

To infer the population properties we are interested in, we employ the typical hierarchical Bayesian inference framework used in compact object population studies. In particular, we formulate the total hyper-posterior following the same prescriptions in \citet{2210.12834} Appendix C as:

\begin{align}
    \text{log} \quad p(\Lambda, \lambda | {d_i}) \propto \sum_{i=1}{N_\text{obs}} log \left[ \frac{1}{K_i} \sum_{j=1}^{K_i} \frac{p(\theta^{i,j} | \Lambda^q) p(z^{i,j} | \lambda)}{\pi (\theta^{i,j}, z^{i,j})} \right] \\ - N_\text{obs} \text{log} \mu(\Lambda, \lambda) + \frac{3N_\text{obs}+N^2_{\text{obs}}}{2N_{\text{eff}}} + \mathbb{O}(N^{-2}_{\text{eff}})
\end{align}

though with some slight differences to account for our latent variable formalism. 